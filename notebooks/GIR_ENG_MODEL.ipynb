{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "Z-jQZ1yAkLW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers sentencepiece datasets\n",
        "! pip install tqdm\n",
        "! pip install torch\n",
        "!pip install sacrebleu\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCHwbU3VkNW5",
        "outputId": "f5eeb2f9-0d4a-4bad-b6c4-a847b8edd8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "KWa86oJpkmGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation\n"
      ],
      "metadata": {
        "id": "3FsgBDXukH_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback,\n",
        "    AutoConfig,\n",
        "    TrainerCallback\n",
        ")\n",
        "import evaluate\n",
        "from typing import Dict, List, Optional, Union\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"training.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set up device and seed for reproducibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Model and configuration parameters\n",
        "base_model_name = \"google/mt5-small\"\n",
        "model_name = \"JMwagunda/GIR-ENG-MODEL\"\n",
        "repo_id = \"JMwagunda/GIR-ENG-MODEL\"\n",
        "output_dir = repo_id\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "learning_rate = 3e-5\n",
        "weight_decay = 0.03\n",
        "num_epochs = 60\n",
        "source_lang = \"sw\"\n",
        "target_lang = \"en\"  # Nyf = Giriama language code\n",
        "save_total_limit = 3\n",
        "gradient_accumulation_steps = 4\n",
        "max_grad_norm = 0.5  # Gradient clipping\n",
        "warmup_ratio = 0.15\n",
        "early_stopping_patience = 5\n",
        "\n",
        "# Language tokens\n",
        "lang_tokens = {\n",
        "    'sw': '<sw>',\n",
        "    'en': '<en>'\n",
        "}\n",
        "\n",
        "# repo_id = \"Lingua-Connect/SWA_TrainerImproved\"  # Your Hub repository ID\n",
        "\n",
        "# Try to download the latest checkpoint from Hub\n",
        "try:\n",
        "    # Load model config\n",
        "    model_config = AutoConfig.from_pretrained(model_name)\n",
        "    # Add dropout for regularization\n",
        "    model_config.dropout_rate = 0.2\n",
        "\n",
        "    # Load the model and tokenizer from the downloaded checkpoint\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    print(\"Successfully loaded model and tokenizer from Hub checkpoint\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"No checkpoint found or error loading from Hub: {e}\")\n",
        "    print(\"Loading base model instead...\")\n",
        "\n",
        "    # Fallback to loading the base model\n",
        "    model_config = AutoConfig.from_pretrained(model_name)\n",
        "    model_config.dropout_rate = 0.2  # Add dropout\n",
        "\n",
        "    # Fallback to loading the base model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    special_tokens = {'additional_special_tokens': list(lang_tokens.values())}\n",
        "    tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Create custom callback for monitoring and debugging\n",
        "class MonitorCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.step_times = []\n",
        "        self.last_time = time.time()\n",
        "        self.step_loss = []\n",
        "\n",
        "    def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and \"loss\" in logs:\n",
        "            # Track loss value\n",
        "            current_loss = logs[\"loss\"]\n",
        "            self.step_loss.append(current_loss)\n",
        "\n",
        "            # Check for NaN or Inf\n",
        "            if math.isnan(current_loss) or math.isinf(current_loss):\n",
        "                logger.warning(f\"WARNING: Abnormal loss detected: {current_loss}\")\n",
        "\n",
        "                # Check model weights for NaN\n",
        "                for name, param in trainer.model.named_parameters():\n",
        "                    if torch.isnan(param).any() or torch.isinf(param).any():\n",
        "                        logger.warning(f\"NaN or Inf found in parameter {name}\")\n",
        "\n",
        "            # Track step time\n",
        "            current_time = time.time()\n",
        "            step_time = current_time - self.last_time\n",
        "            self.step_times.append(step_time)\n",
        "            self.last_time = current_time\n",
        "\n",
        "            # Report average step time and memory every 50 steps\n",
        "            if state.global_step % 50 == 0:\n",
        "                avg_step_time = sum(self.step_times[-50:]) / min(50, len(self.step_times))\n",
        "                logger.info(f\"Step {state.global_step}: Avg step time = {avg_step_time:.3f}s, Loss = {current_loss:.4f}\")\n",
        "\n",
        "                # Reset step times after reporting\n",
        "                if len(self.step_times) > 100:\n",
        "                    self.step_times = self.step_times[-50:]\n",
        "                if len(self.step_loss) > 100:\n",
        "                    self.step_loss = self.step_loss[-50:]\n",
        "\n",
        "                # Report memory usage if on CUDA\n",
        "                if torch.cuda.is_available():\n",
        "                    mem_allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "                    mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "                    logger.info(f\"GPU Memory: Allocated = {mem_allocated:.1f}MB, Reserved = {mem_reserved:.1f}MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pB8nKYa_hzp",
        "outputId": "a86deeb3-4bce-478c-ceab-d7c1095606b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded model and tokenizer from Hub checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved early stopping callback with more detailed logging\n",
        "class DetailedEarlyStoppingCallback(EarlyStoppingCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        metric_to_check = args.metric_for_best_model\n",
        "        if not metric_to_check.startswith(\"eval_\"):\n",
        "            metric_to_check = f\"eval_{metric_to_check}\"\n",
        "\n",
        "        metric_value = metrics.get(metric_to_check)\n",
        "\n",
        "        logger.info(f\"Early stopping metric '{metric_to_check}' value: {metric_value}\")\n",
        "        logger.info(f\"Best value so far: {state.best_metric}\")\n",
        "        logger.info(f\"No improvement counter: {self.early_stopping_patience_counter}\")\n",
        "\n",
        "        # Call the parent class method\n",
        "        super().on_evaluate(args, state, control, metrics, **kwargs)\n",
        "\n",
        "        if control.should_training_stop:\n",
        "            logger.warning(\"Early stopping triggered! Training will stop.\")"
      ],
      "metadata": {
        "id": "Cc6cxjgDGNU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "55PkNRO5kP7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load preprocessed data or process it again if needed\n",
        "def load_or_preprocess_data():\n",
        "\n",
        "        # Load the dataset\n",
        "        ds = load_dataset('Lingua-Connect/English-Giriama-Dataset')\n",
        "        split_datasets = ds[\"train\"].train_test_split(train_size=0.9, seed=seed)\n",
        "        split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
        "\n",
        "        logger.info(f\"Dataset loaded: {len(split_datasets['train'])} train, {len(split_datasets['validation'])} validation\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        # Define preprocessing function\n",
        "        def preprocess_function(examples):\n",
        "            # Prepare input texts with prefix\n",
        "            source_prefix = f\"translate {source_lang} to {target_lang}: \"\n",
        "            inputs = [source_prefix + sw for sw in examples['Giriama Translation'] if sw is not None]\n",
        "            targets = [str(en) for en in examples['English Sentence'] if en is not None]\n",
        "\n",
        "            # Check if inputs and targets have the same length after filtering\n",
        "            if len(inputs) != len(targets):\n",
        "                # Handle the case where they have different lengths\n",
        "                min_len = min(len(inputs), len(targets))\n",
        "                inputs = inputs[:min_len]\n",
        "                targets = targets[:min_len]\n",
        "\n",
        "            # Tokenize inputs\n",
        "            model_inputs = tokenizer(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=None\n",
        "            )\n",
        "\n",
        "            # Tokenize targets\n",
        "            labels = tokenizer(\n",
        "                targets,\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=None\n",
        "            )\n",
        "\n",
        "            # Add labels to model inputs\n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "            # Replace pad token id with -100 in labels so it's ignored in loss computation\n",
        "            for i in range(len(model_inputs[\"labels\"])):\n",
        "                pad_mask = [token == tokenizer.pad_token_id for token in model_inputs[\"labels\"][i]]\n",
        "                model_inputs[\"labels\"][i] = [\n",
        "                    -100 if mask else token\n",
        "                    for mask, token in zip(pad_mask, model_inputs[\"labels\"][i])\n",
        "                ]\n",
        "\n",
        "            return model_inputs\n",
        "\n",
        "        # Process datasets\n",
        "        logger.info(\"Processing datasets...\")\n",
        "        train_dataset = split_datasets[\"train\"].map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            remove_columns=split_datasets[\"train\"].column_names,\n",
        "            desc=\"Preprocessing training dataset\"\n",
        "        )\n",
        "\n",
        "        validation_dataset = split_datasets[\"validation\"].map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            remove_columns=split_datasets[\"validation\"].column_names,\n",
        "            desc=\"Preprocessing validation dataset\"\n",
        "        )\n",
        "\n",
        "        return train_dataset, validation_dataset"
      ],
      "metadata": {
        "id": "J-KTrxfjkRc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_dataset, validation_dataset = load_or_preprocess_data()\n",
        "\n",
        "# Load model and tokenizer\n",
        "logger.info(f\"Loading model: {model_name}\")\n",
        "model_config = AutoConfig.from_pretrained(model_name)\n",
        "model_config.dropout_rate = 0.2  # Add dropout for regularization\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure pad_token_id is set correctly\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# # Move model to device\n",
        "# model = model.to(device)\n",
        "# logger.info(f\"Model loaded with {model.num_parameters():,} parameters\")\n",
        "\n",
        "# Initialize output layer weights with small values for numerical stability\n",
        "for name, param in model.named_parameters():\n",
        "    if \"decoder\" in name and \"dense\" in name:\n",
        "        logger.info(f\"Initializing {name} with small values\")\n",
        "        torch.nn.init.normal_(param, mean=0.0, std=0.02)\n",
        "\n",
        "# Custom optimizer setup with layer-wise learning rate decay\n",
        "def get_optimizer(model, lr):\n",
        "    decay_parameters = [p for n, p in model.named_parameters() if \"LayerNorm\" not in n and p.requires_grad]\n",
        "    no_decay_parameters = [p for n, p in model.named_parameters() if \"LayerNorm\" in n and p.requires_grad]\n",
        "\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": decay_parameters, \"weight_decay\": weight_decay, \"lr\": lr},\n",
        "        {\"params\": no_decay_parameters, \"weight_decay\": 0.0, \"lr\": lr}\n",
        "    ]\n",
        "\n",
        "    return AdamW(optimizer_grouped_parameters)\n",
        "\n",
        "# Prepare data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=\"max_length\",\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "X9DAsdGdW-iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load metric for evaluation\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "zsYB2ZlnXSXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "4CK3fEYxkR4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # In case the model returns more than the prediction logits\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # Debug information - use print in addition to logger\n",
        "    print(f\"Prediction shape: {preds.shape}, Labels shape: {labels.shape}\")\n",
        "    logger.info(f\"Prediction shape: {preds.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "    try:\n",
        "        # Check vocabulary boundaries\n",
        "        vocab_size = tokenizer.vocab_size\n",
        "        print(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
        "        logger.info(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
        "\n",
        "        # Replace token IDs that are out of vocabulary range with pad token ID\n",
        "        invalid_indices = np.where((preds >= vocab_size) | (preds < 0))\n",
        "        if invalid_indices[0].size > 0:\n",
        "            print(f\"Found {invalid_indices[0].size} token IDs outside vocab range. Replacing with pad token.\")\n",
        "            logger.warning(f\"Found {invalid_indices[0].size} token IDs outside vocab range. Replacing with pad token.\")\n",
        "            preds[invalid_indices] = tokenizer.pad_token_id\n",
        "\n",
        "        # Decode predictions\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "        # Handle labels: replace -100 with pad token ID and clip to valid range\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        invalid_label_indices = np.where((labels >= vocab_size) | (labels < 0))\n",
        "        if invalid_label_indices[0].size > 0:\n",
        "            print(f\"Found {invalid_label_indices[0].size} label IDs outside vocab range. Replacing with pad token.\")\n",
        "            logger.warning(f\"Found {invalid_label_indices[0].size} label IDs outside vocab range. Replacing with pad token.\")\n",
        "            labels[invalid_label_indices] = tokenizer.pad_token_id\n",
        "\n",
        "        # Decode labels\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Post-processing\n",
        "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "        # Debug output - print some examples with both print and logger\n",
        "        print(\"\\n===== PREDICTION EXAMPLES =====\")\n",
        "        for i in range(min(3, len(decoded_preds))):\n",
        "            print(f\"Pred[{i}]: {decoded_preds[i][:100]}...\")\n",
        "            print(f\"Label[{i}]: {decoded_labels[i][0][:100]}...\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            logger.info(f\"Pred[{i}]: {decoded_preds[i][:100]}...\")\n",
        "            logger.info(f\"Label[{i}]: {decoded_labels[i][0][:100]}...\")\n",
        "\n",
        "        # Ensure these examples are flushed to output\n",
        "        import sys\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Compute BLEU score\n",
        "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "        # Add generation length\n",
        "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "        # Format results\n",
        "        formatted_result = {\n",
        "            \"bleu\": round(result[\"score\"], 4),\n",
        "            \"gen_len\": round(result[\"gen_len\"], 4)\n",
        "        }\n",
        "\n",
        "        # Print final metrics\n",
        "        print(f\"\\nMetrics: BLEU = {formatted_result['bleu']}, Gen Length = {formatted_result['gen_len']}\")\n",
        "\n",
        "        return formatted_result\n",
        "\n",
        "    except Exception as e:\n",
        "        # More detailed error logging\n",
        "        error_msg = f\"Error in compute_metrics: {e}\"\n",
        "        print(error_msg)\n",
        "        logger.error(error_msg)\n",
        "\n",
        "        import traceback\n",
        "        tb = traceback.format_exc()\n",
        "        print(f\"Traceback: {tb}\")\n",
        "        logger.error(f\"Traceback: {tb}\")\n",
        "\n",
        "        # Return zeros to prevent training from crashing\n",
        "        return {\"bleu\": 0.0, \"gen_len\": 0.0}"
      ],
      "metadata": {
        "id": "payAGyBtkS0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    # eval_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    # save_steps=100,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=weight_decay,\n",
        "    save_total_limit=save_total_limit,\n",
        "    num_train_epochs=num_epochs,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,  # Disable mixed precision initially for stability\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=repo_id,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"bleu\",\n",
        "    greater_is_better=True,\n",
        "    resume_from_checkpoint=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    logging_dir=f\"./logs\",\n",
        "    logging_steps=10,\n",
        "    generation_max_length=max_length,\n",
        "    generation_num_beams=4,\n",
        "    label_smoothing_factor=0.15,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = get_optimizer(model, learning_rate)\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(early_stopping_patience=early_stopping_patience),\n",
        "        MonitorCallback()\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "gXLyE6CYkVDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de545be-129b-4d54-e133-96707edced62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-e26e9b1c184e>:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial evaluation\n",
        "print(\"\\nRunning initial evaluation...\")\n",
        "initial_eval_results = trainer.evaluate(max_length=max_length)\n",
        "print(f\"Initial evaluation results: {initial_eval_results}\")"
      ],
      "metadata": {
        "id": "W4RZHJ11bBZW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "17767e0c-6a89-4b52-b14d-8289ed5095cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running initial evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [49/49 00:57]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 27872 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 126), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58905\n",
            "Found 27872 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Jesus said to them A prophet was not born in his own country...\n",
            "Label[0]: Jesus had said before that a prophet is not respected in his own country...\n",
            "--------------------------------------------------\n",
            "Pred[1]: Peter baptized them again He said to them I dont know him...\n",
            "Label[1]: Again Peter said he was never with Jesus He said I swear to God I dont know the man...\n",
            "--------------------------------------------------\n",
            "Pred[2]: Peter said No If all of you leave you I will not leave you...\n",
            "Label[2]: Peter answered All the other followers may lose their faith in you But my faith will never be shaken...\n",
            "--------------------------------------------------\n",
            "\n",
            "Metrics: BLEU = 14.8574, Gen Length = 47.1547\n",
            "Initial evaluation results: {'eval_loss': 3.2401461601257324, 'eval_model_preparation_time': 0.0063, 'eval_bleu': 14.8574, 'eval_gen_len': 47.1547, 'eval_runtime': 61.1556, 'eval_samples_per_second': 12.787, 'eval_steps_per_second': 0.801}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the path if checkpoint was downloaded, otherwise let it default to None\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wtYVt-q_bBWx",
        "outputId": "5a1287cd-eb54-426d-dbaf-544adb0f5866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='6600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 770/6600 25:54 < 3:16:42, 0.49 it/s, Epoch 7/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.074100</td>\n",
              "      <td>3.191496</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>15.463800</td>\n",
              "      <td>47.745500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.086500</td>\n",
              "      <td>3.185565</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>15.482800</td>\n",
              "      <td>47.799200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.053100</td>\n",
              "      <td>3.181058</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>15.382400</td>\n",
              "      <td>47.685400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.078300</td>\n",
              "      <td>3.179790</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>15.128500</td>\n",
              "      <td>47.147100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.043600</td>\n",
              "      <td>3.177676</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>15.144900</td>\n",
              "      <td>46.555000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.051800</td>\n",
              "      <td>3.177946</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>15.392300</td>\n",
              "      <td>47.433500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.042900</td>\n",
              "      <td>3.178887</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>15.147300</td>\n",
              "      <td>47.613800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 22688 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 118), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58905\n",
            "Found 22688 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: I want you to know how to live with me against this But you see me fighting behind me and this is wh...\n",
            "Label[0]: You saw the difficulties I had to face and you hear that I am still having troubles Now you must fac...\n",
            "--------------------------------------------------\n",
            "Pred[1]: They began to accuse him They said to him Are you the king of the Jews...\n",
            "Label[1]: Then they began shouting Welcome king of the Jews...\n",
            "--------------------------------------------------\n",
            "Pred[2]: Come to me about all you and that you have given to me is a burden and I will give you it...\n",
            "Label[2]: Come to me all of you who are tired from the heavy burden you have been forced to carry I will give ...\n",
            "--------------------------------------------------\n",
            "\n",
            "Metrics: BLEU = 15.4638, Gen Length = 47.7455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 29248 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58905\n",
            "Found 29248 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: It is the same with those who are raised from death The body that is buried is the body that will di...\n",
            "Label[0]: It will be the same when those who have died are raised to life The body that is planted in the grav...\n",
            "--------------------------------------------------\n",
            "Pred[1]: So we sent Jesus to the place where we can find our own guard in his own rock and helped us...\n",
            "Label[1]: So we should go to Jesus outside the camp and accept the same shame that he had...\n",
            "--------------------------------------------------\n",
            "Pred[2]: There are heavens bodies and earthly bodies The goodness of heavens is something else and the goodne...\n",
            "Label[2]: Also there are heavenly bodies and earthly bodies But the beauty of the heavenly bodies is one kind ...\n",
            "--------------------------------------------------\n",
            "\n",
            "Metrics: BLEU = 15.4828, Gen Length = 47.7992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 30920 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58905\n",
            "Found 30920 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: God has the power to give you more than that you will always have to have everything you need He wil...\n",
            "Label[0]: And God can give you more blessings than you need and you will always have plenty of everything You ...\n",
            "--------------------------------------------------\n",
            "Pred[1]: My servant is a very sick and very small servant That is why he is sent and my servant is very sick...\n",
            "Label[1]: The officer said Lord my servant is very sick at home in bed He cant move his body and has much pain...\n",
            "--------------------------------------------------\n",
            "Pred[2]: You have heard that when everyone was told When you swear before the Lord dont obey everything you h...\n",
            "Label[2]: You have heard that it was said to our people long ago When you make a vow you must not break your p...\n",
            "--------------------------------------------------\n",
            "\n",
            "Metrics: BLEU = 15.3824, Gen Length = 47.6854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 31156 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58905\n",
            "Found 31156 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: David himself calls the Lord He can be his son The meeting was very happy and very happy...\n",
            "Label[0]: David himself calls the Messiah Lord So how can the Messiah be Davids son Many people listened to Je...\n",
            "--------------------------------------------------\n",
            "Pred[1]: To the Jews I became like a Jew so that I could obey the Jews I did not obey their law as if I were ...\n",
            "Label[1]: To the Jews I became like a Jew so that I could help save Jews I myself am not ruled by the law but ...\n",
            "--------------------------------------------------\n",
            "Pred[2]: If you did not come back I would not be able to hit those who follow the teaching and the sword that...\n",
            "Label[2]: So change your hearts If you dont change I will come to you quickly and fight against these people w...\n",
            "--------------------------------------------------\n",
            "\n",
            "Metrics: BLEU = 15.1285, Gen Length = 47.1471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 28948 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 125), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58905\n",
            "Found 28948 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: But Jesus went to the Mount of Olives...\n",
            "Label[0]: Jesus went to the Mount of Olives...\n",
            "--------------------------------------------------\n",
            "Pred[1]: They were very afraid of this So they threw away the four anchors and threw the ships into the water...\n",
            "Label[1]: The sailors were afraid that we would hit the rocks so they threw four anchors into the water Then t...\n",
            "--------------------------------------------------\n",
            "Pred[2]: Some people said He is a good man But others said He is a follower...\n",
            "Label[2]: There was a large group of people there Many of them were talking secretly to each other about Jesus...\n",
            "--------------------------------------------------\n",
            "\n",
            "Metrics: BLEU = 15.1449, Gen Length = 46.555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 27868 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58905\n",
            "Found 27868 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: The second servant came and said Sir everythird of the golden feet had a golden bag...\n",
            "Label[0]: The second servant said Sir with your one bag of money I earned five bags...\n",
            "--------------------------------------------------\n",
            "Pred[1]: But now I tell you this is not anyone who has the name of the brother Then he is a follower or a fol...\n",
            "Label[1]: I wrote to you in my letter that you should not associate with people who sin sexually...\n",
            "--------------------------------------------------\n",
            "Pred[2]: This means Abraham had two sons One of his sons married a slave woman and another woman married her ...\n",
            "Label[2]: The Scriptures say that Abraham had two sons The mother of one son was a slave woman and the mother ...\n",
            "--------------------------------------------------\n",
            "\n",
            "Metrics: BLEU = 15.3923, Gen Length = 47.4335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 29668 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58905\n",
            "Found 29668 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: So the people were afraid of this because of Jesus...\n",
            "Label[0]: So the people did not agree with each other about Jesus...\n",
            "--------------------------------------------------\n",
            "Pred[1]: But you teach that people will give their father or mother And they have helped you understand what ...\n",
            "Label[1]: But you teach that a person can say to their father or mother I have something I could use to help y...\n",
            "--------------------------------------------------\n",
            "Pred[2]: God is the one who is our Savior and glory and power and power because of our Lord Jesus Christ...\n",
            "Label[2]: He is the only God the one who saves us To him be glory greatness power and authority through Jesus ...\n",
            "--------------------------------------------------\n",
            "\n",
            "Metrics: BLEU = 15.1473, Gen Length = 47.6138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=770, training_loss=3.0636291528677013, metrics={'train_runtime': 1556.0814, 'train_samples_per_second': 271.335, 'train_steps_per_second': 4.241, 'total_flos': 1669799557988352.0, 'train_loss': 3.0636291528677013, 'epoch': 7.0})"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUuvh8VptQf4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}