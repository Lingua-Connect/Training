{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60f9c8ca815f4a2ab70a9da90265c47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_77094a1d93754dcb91c4b0f472abc472"
          }
        },
        "6dc22aa00a324d6ea2b3b50ef84d1c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51635b8e05b84fb48b74d0d7e99dd5a1",
            "placeholder": "​",
            "style": "IPY_MODEL_ef0af21b204143b3993853dc9fffe09d",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "bbe0b186240444e4bbd88741c9c48b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c6ff4a20b0864de98fd993d15fc52021",
            "placeholder": "​",
            "style": "IPY_MODEL_ecc2801c7fb54283abdf763d7a6ba0a2",
            "value": ""
          }
        },
        "aa2b3b64b4e04c09868c07768738011a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_b1072b235ae9478e9c50121b017c2ec7",
            "style": "IPY_MODEL_e2e13b3f88bd4124bf1caeb24d424f7c",
            "value": true
          }
        },
        "10f4a934f08e4ff1b22edea7eade681c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5a0b7274018e482ab133cec3b5d1fddc",
            "style": "IPY_MODEL_b508089d8a274788889b15053f2f8140",
            "tooltip": ""
          }
        },
        "c5e791740e4f4d65aa9a5a4ece63d387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e18e33f222ad4dd5b761064c259080bb",
            "placeholder": "​",
            "style": "IPY_MODEL_03c495479f594f57b3646c8ba872730d",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "77094a1d93754dcb91c4b0f472abc472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "51635b8e05b84fb48b74d0d7e99dd5a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef0af21b204143b3993853dc9fffe09d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6ff4a20b0864de98fd993d15fc52021": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc2801c7fb54283abdf763d7a6ba0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1072b235ae9478e9c50121b017c2ec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2e13b3f88bd4124bf1caeb24d424f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a0b7274018e482ab133cec3b5d1fddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b508089d8a274788889b15053f2f8140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e18e33f222ad4dd5b761064c259080bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c495479f594f57b3646c8ba872730d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b4a35085c0340ccb115a0a632ff3e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_514a4245a806481fbea4a7e38e90dc7d",
            "placeholder": "​",
            "style": "IPY_MODEL_7fbea0efd284447e95600fc4c937414b",
            "value": "Connecting..."
          }
        },
        "514a4245a806481fbea4a7e38e90dc7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fbea0efd284447e95600fc4c937414b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers sentencepiece datasets\n",
        "! pip install tqdm\n",
        "! pip install torch\n",
        "!pip install sacrebleu\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5S__fBLZ8qM",
        "outputId": "34844122-f480-4f62-c059-2c2e295fe0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "60f9c8ca815f4a2ab70a9da90265c47c",
            "6dc22aa00a324d6ea2b3b50ef84d1c7f",
            "bbe0b186240444e4bbd88741c9c48b35",
            "aa2b3b64b4e04c09868c07768738011a",
            "10f4a934f08e4ff1b22edea7eade681c",
            "c5e791740e4f4d65aa9a5a4ece63d387",
            "77094a1d93754dcb91c4b0f472abc472",
            "51635b8e05b84fb48b74d0d7e99dd5a1",
            "ef0af21b204143b3993853dc9fffe09d",
            "c6ff4a20b0864de98fd993d15fc52021",
            "ecc2801c7fb54283abdf763d7a6ba0a2",
            "b1072b235ae9478e9c50121b017c2ec7",
            "e2e13b3f88bd4124bf1caeb24d424f7c",
            "5a0b7274018e482ab133cec3b5d1fddc",
            "b508089d8a274788889b15053f2f8140",
            "e18e33f222ad4dd5b761064c259080bb",
            "03c495479f594f57b3646c8ba872730d",
            "2b4a35085c0340ccb115a0a632ff3e94",
            "514a4245a806481fbea4a7e38e90dc7d",
            "7fbea0efd284447e95600fc4c937414b"
          ]
        },
        "id": "oi6cLN-eZ87l",
        "outputId": "114da7f5-9331-4500-a296-10b62967cc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60f9c8ca815f4a2ab70a9da90265c47c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback,\n",
        "    TrainerCallback\n",
        ")\n",
        "import evaluate\n",
        "from typing import Dict, List, Optional, Union\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"training.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set up device and seed for reproducibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Model and configuration parameters\n",
        "base_model_name = \"google/mt5-small\"\n",
        "model_name = \"JMwagunda/ENG-GIR-MODEL\"\n",
        "repo_id = \"JMwagunda/ENG-GIR-MODEL\"\n",
        "output_dir = repo_id\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "learning_rate = 5e-5\n",
        "weight_decay = 0.01\n",
        "num_epochs = 40\n",
        "source_lang = \"en\"\n",
        "target_lang = \"sw\"  # Nyf = Giriama language code\n",
        "save_total_limit = 3\n",
        "gradient_accumulation_steps = 4\n",
        "max_grad_norm = 1.0  # Gradient clipping\n",
        "warmup_ratio = 0.1\n",
        "early_stopping_patience = 3\n",
        "\n",
        "# Language tokens\n",
        "lang_tokens = {\n",
        "    'en': '<en>',\n",
        "    'sw': '<sw>'\n",
        "}\n",
        "\n",
        "# repo_id = \"Lingua-Connect/SWA_TrainerImproved\"  # Your Hub repository ID\n",
        "\n",
        "# Try to download the latest checkpoint from Hub\n",
        "try:\n",
        "    # Load the model and tokenizer from the downloaded checkpoint\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    print(\"Successfully loaded model and tokenizer from Hub checkpoint\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"No checkpoint found or error loading from Hub: {e}\")\n",
        "    print(\"Loading base model instead...\")\n",
        "\n",
        "    # Fallback to loading the base model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    special_tokens = {'additional_special_tokens': list(lang_tokens.values())}\n",
        "    tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Create custom callback for monitoring and debugging\n",
        "class MonitorCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.step_times = []\n",
        "        self.last_time = time.time()\n",
        "        self.step_loss = []\n",
        "\n",
        "    def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and \"loss\" in logs:\n",
        "            # Track loss value\n",
        "            current_loss = logs[\"loss\"]\n",
        "            self.step_loss.append(current_loss)\n",
        "\n",
        "            # Check for NaN or Inf\n",
        "            if math.isnan(current_loss) or math.isinf(current_loss):\n",
        "                logger.warning(f\"WARNING: Abnormal loss detected: {current_loss}\")\n",
        "\n",
        "                # Check model weights for NaN\n",
        "                for name, param in trainer.model.named_parameters():\n",
        "                    if torch.isnan(param).any() or torch.isinf(param).any():\n",
        "                        logger.warning(f\"NaN or Inf found in parameter {name}\")\n",
        "\n",
        "            # Track step time\n",
        "            current_time = time.time()\n",
        "            step_time = current_time - self.last_time\n",
        "            self.step_times.append(step_time)\n",
        "            self.last_time = current_time\n",
        "\n",
        "            # Report average step time and memory every 50 steps\n",
        "            if state.global_step % 50 == 0:\n",
        "                avg_step_time = sum(self.step_times[-50:]) / min(50, len(self.step_times))\n",
        "                logger.info(f\"Step {state.global_step}: Avg step time = {avg_step_time:.3f}s, Loss = {current_loss:.4f}\")\n",
        "\n",
        "                # Reset step times after reporting\n",
        "                if len(self.step_times) > 100:\n",
        "                    self.step_times = self.step_times[-50:]\n",
        "                if len(self.step_loss) > 100:\n",
        "                    self.step_loss = self.step_loss[-50:]\n",
        "\n",
        "                # Report memory usage if on CUDA\n",
        "                if torch.cuda.is_available():\n",
        "                    mem_allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "                    mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "                    logger.info(f\"GPU Memory: Allocated = {mem_allocated:.1f}MB, Reserved = {mem_reserved:.1f}MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KCG2GhgZ9BV",
        "outputId": "ab3892d9-2f64-4839-c5b9-5e82525bac39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded model and tokenizer from Hub checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load preprocessed data or process it again if needed\n",
        "def load_or_preprocess_data():\n",
        "\n",
        "        # Load the dataset\n",
        "        ds = load_dataset('Lingua-Connect/English-Giriama-Dataset')\n",
        "        split_datasets = ds[\"train\"].train_test_split(train_size=0.9, seed=seed)\n",
        "        split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
        "\n",
        "        logger.info(f\"Dataset loaded: {len(split_datasets['train'])} train, {len(split_datasets['validation'])} validation\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        # Define preprocessing function\n",
        "        def preprocess_function(examples):\n",
        "            # Prepare input texts with prefix\n",
        "            source_prefix = f\"translate {source_lang} to {target_lang}: \"\n",
        "            inputs = [source_prefix + en for en in examples['English Sentence'] if en is not None]\n",
        "            targets = [str(sw) for sw in examples['Giriama Translation'] if sw is not None]\n",
        "\n",
        "            # Check if inputs and targets have the same length after filtering\n",
        "            if len(inputs) != len(targets):\n",
        "                # Handle the case where they have different lengths\n",
        "                min_len = min(len(inputs), len(targets))\n",
        "                inputs = inputs[:min_len]\n",
        "                targets = targets[:min_len]\n",
        "\n",
        "            # Tokenize inputs\n",
        "            model_inputs = tokenizer(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=None\n",
        "            )\n",
        "\n",
        "            # Tokenize targets\n",
        "            labels = tokenizer(\n",
        "                targets,\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=None\n",
        "            )\n",
        "\n",
        "            # Add labels to model inputs\n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "            # Replace pad token id with -100 in labels so it's ignored in loss computation\n",
        "            for i in range(len(model_inputs[\"labels\"])):\n",
        "                pad_mask = [token == tokenizer.pad_token_id for token in model_inputs[\"labels\"][i]]\n",
        "                model_inputs[\"labels\"][i] = [\n",
        "                    -100 if mask else token\n",
        "                    for mask, token in zip(pad_mask, model_inputs[\"labels\"][i])\n",
        "                ]\n",
        "\n",
        "            return model_inputs\n",
        "\n",
        "        # Process datasets\n",
        "        logger.info(\"Processing datasets...\")\n",
        "        train_dataset = split_datasets[\"train\"].map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            remove_columns=split_datasets[\"train\"].column_names,\n",
        "            desc=\"Preprocessing training dataset\"\n",
        "        )\n",
        "\n",
        "        validation_dataset = split_datasets[\"validation\"].map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            remove_columns=split_datasets[\"validation\"].column_names,\n",
        "            desc=\"Preprocessing validation dataset\"\n",
        "        )\n",
        "\n",
        "        return train_dataset, validation_dataset"
      ],
      "metadata": {
        "id": "PERbM1CYZ9EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_dataset, validation_dataset = load_or_preprocess_data()\n",
        "\n",
        "# Load model and tokenizer\n",
        "logger.info(f\"Loading model: {model_name}\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure pad_token_id is set correctly\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# # Move model to device\n",
        "# model = model.to(device)\n",
        "# logger.info(f\"Model loaded with {model.num_parameters():,} parameters\")\n",
        "\n",
        "# Initialize output layer weights with small values for numerical stability\n",
        "for name, param in model.named_parameters():\n",
        "    if \"decoder\" in name and \"dense\" in name:\n",
        "        logger.info(f\"Initializing {name} with small values\")\n",
        "        torch.nn.init.normal_(param, mean=0.0, std=0.02)\n",
        "\n",
        "# Prepare data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=\"max_length\",\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "T9-t3-uFZ9HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load metric for evaluation\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "LmTrCmAIZ9Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # In case the model returns more than the prediction logits\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # Debug information - use print in addition to logger\n",
        "    print(f\"Prediction shape: {preds.shape}, Labels shape: {labels.shape}\")\n",
        "    logger.info(f\"Prediction shape: {preds.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "    try:\n",
        "        # Check vocabulary boundaries\n",
        "        vocab_size = tokenizer.vocab_size\n",
        "        print(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
        "        logger.info(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
        "\n",
        "        # Replace token IDs that are out of vocabulary range with pad token ID\n",
        "        invalid_indices = np.where((preds >= vocab_size) | (preds < 0))\n",
        "        if invalid_indices[0].size > 0:\n",
        "            print(f\"Found {invalid_indices[0].size} token IDs outside vocab range. Replacing with pad token.\")\n",
        "            logger.warning(f\"Found {invalid_indices[0].size} token IDs outside vocab range. Replacing with pad token.\")\n",
        "            preds[invalid_indices] = tokenizer.pad_token_id\n",
        "\n",
        "        # Decode predictions\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "        # Handle labels: replace -100 with pad token ID and clip to valid range\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        invalid_label_indices = np.where((labels >= vocab_size) | (labels < 0))\n",
        "        if invalid_label_indices[0].size > 0:\n",
        "            print(f\"Found {invalid_label_indices[0].size} label IDs outside vocab range. Replacing with pad token.\")\n",
        "            logger.warning(f\"Found {invalid_label_indices[0].size} label IDs outside vocab range. Replacing with pad token.\")\n",
        "            labels[invalid_label_indices] = tokenizer.pad_token_id\n",
        "\n",
        "        # Decode labels\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Post-processing\n",
        "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "        # Debug output - print some examples with both print and logger\n",
        "        print(\"\\n===== PREDICTION EXAMPLES =====\")\n",
        "        for i in range(min(3, len(decoded_preds))):\n",
        "            print(f\"Pred[{i}]: {decoded_preds[i][:100]}...\")\n",
        "            print(f\"Label[{i}]: {decoded_labels[i][0][:100]}...\")\n",
        "            # print(\"-\" * 50)\n",
        "\n",
        "            logger.info(f\"Pred[{i}]: {decoded_preds[i][:100]}...\")\n",
        "            logger.info(f\"Label[{i}]: {decoded_labels[i][0][:100]}...\")\n",
        "\n",
        "        # Ensure these examples are flushed to output\n",
        "        import sys\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Compute BLEU score\n",
        "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "        # Add generation length\n",
        "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "        # Format results\n",
        "        formatted_result = {\n",
        "            \"bleu\": round(result[\"score\"], 4),\n",
        "            \"gen_len\": round(result[\"gen_len\"], 4)\n",
        "        }\n",
        "\n",
        "        # Print final metrics\n",
        "        print(f\"\\nMetrics: BLEU = {formatted_result['bleu']}, Gen Length = {formatted_result['gen_len']}\")\n",
        "\n",
        "        return formatted_result\n",
        "\n",
        "    except Exception as e:\n",
        "        # More detailed error logging\n",
        "        error_msg = f\"Error in compute_metrics: {e}\"\n",
        "        print(error_msg)\n",
        "        logger.error(error_msg)\n",
        "\n",
        "        import traceback\n",
        "        tb = traceback.format_exc()\n",
        "        print(f\"Traceback: {tb}\")\n",
        "        logger.error(f\"Traceback: {tb}\")\n",
        "\n",
        "        # Return zeros to prevent training from crashing\n",
        "        return {\"bleu\": 0.0, \"gen_len\": 0.0}"
      ],
      "metadata": {
        "id": "FPNh21Y-aIed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    # eval_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    # save_steps=100,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=weight_decay,\n",
        "    save_total_limit=save_total_limit,\n",
        "    num_train_epochs=num_epochs,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,  # Disable mixed precision initially for stability\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=repo_id,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"bleu\",\n",
        "    greater_is_better=True,\n",
        "    resume_from_checkpoint=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    logging_dir=f\"./logs\",\n",
        "    logging_steps=10,\n",
        "    generation_max_length=max_length,\n",
        "    generation_num_beams=4,\n",
        "    label_smoothing_factor=0.1,\n",
        "    lr_scheduler_type=\"polynomial\",\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(early_stopping_patience=early_stopping_patience),\n",
        "        MonitorCallback()\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrt1hlUUaIcg",
        "outputId": "2a5c621e-e275-4301-8990-ea3bfca2f256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-ff931c5b4ea9>:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial evaluation\n",
        "print(\"\\nRunning initial evaluation...\")\n",
        "initial_eval_results = trainer.evaluate(max_length=max_length)\n",
        "print(f\"Initial evaluation results: {initial_eval_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "HRln49GuaIaG",
        "outputId": "d417fb30-0290-40e9-b240-73c9a910bb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running initial evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [49/49 01:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 16126 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 16126 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Jesu arihokala adzagonya kunena kare kukala nabii k'aheshimu kahi za ts'i ya kwakwe mwenye...\n",
            "Label[0]: Kwani Jesu mwenye were waamba Nabii k'aishimiwa kahi za ts'i ya kwao...\n",
            "Pred[1]: P'et'ero akienderera kunena akiamba Hatha simumanya kaheri kaheri...\n",
            "Label[1]: P'et'ero akikanaiza kaheri kwa kuapa akiamba Mwanamulume iye simumanya kamare...\n",
            "Pred[2]: P'et'ero akimudzigidzya akiamba Ndo anafundzi angine osi mandiokala manamukuluhira ela ro kuluhiro r...\n",
            "Label[2]: P'et'ero akimudzigidzya akiamba Hatha kala osi mandakuricha mimi sindakuricha ng'o...\n",
            "\n",
            "Metrics: BLEU = 9.079, Gen Length = 57.1151\n",
            "Initial evaluation results: {'eval_loss': 3.278590679168701, 'eval_model_preparation_time': 0.0147, 'eval_bleu': 9.079, 'eval_gen_len': 57.1151, 'eval_runtime': 70.6712, 'eval_samples_per_second': 11.065, 'eval_steps_per_second': 0.693}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the path if checkpoint was downloaded, otherwise let it default to None\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_AhSZMTeaIUb",
        "outputId": "837e4d04-45af-4c0b-d3e7-3bb2f57deafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1761' max='4400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1761/4400 57:57 < 1:26:56, 0.51 it/s, Epoch 16/40]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.401700</td>\n",
              "      <td>3.266871</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>8.839000</td>\n",
              "      <td>55.728900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.384300</td>\n",
              "      <td>3.283203</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>8.913200</td>\n",
              "      <td>55.796700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.368500</td>\n",
              "      <td>3.278569</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>8.884400</td>\n",
              "      <td>55.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.312300</td>\n",
              "      <td>3.299557</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>8.764700</td>\n",
              "      <td>56.451400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.304300</td>\n",
              "      <td>3.320510</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>8.887200</td>\n",
              "      <td>55.902800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.242300</td>\n",
              "      <td>3.342463</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>8.911000</td>\n",
              "      <td>55.997400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.231400</td>\n",
              "      <td>3.351343</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>9.141600</td>\n",
              "      <td>56.822300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.288200</td>\n",
              "      <td>3.364541</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>8.854500</td>\n",
              "      <td>55.782600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.280700</td>\n",
              "      <td>3.338727</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>9.095200</td>\n",
              "      <td>55.831200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.243600</td>\n",
              "      <td>3.333600</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>9.191900</td>\n",
              "      <td>56.484700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.228400</td>\n",
              "      <td>3.340943</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>9.266300</td>\n",
              "      <td>55.081800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.193400</td>\n",
              "      <td>3.357557</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>9.455400</td>\n",
              "      <td>55.405400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.172800</td>\n",
              "      <td>3.370853</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>9.433400</td>\n",
              "      <td>55.929700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.158700</td>\n",
              "      <td>3.373467</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>9.244300</td>\n",
              "      <td>55.791600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.118400</td>\n",
              "      <td>3.388104</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>9.826800</td>\n",
              "      <td>55.590800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/49 00:25 < 00:38, 0.76 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='98' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [49/49 04:39]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 18100 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 18100 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Jesu arihokala adzagonya kunena kare kukala nabii k'aheshimu kahi za ts'i ya kwakwe mwenye...\n",
            "Label[0]: Kwani Jesu mwenye were waamba Nabii k'aishimiwa kahi za ts'i ya kwao...\n",
            "Pred[1]: K'uzhona zho Mwanawe akikala haho kabila k'adzaumbwa kit'u chochosi...\n",
            "Label[1]: Iye Masihi wakalako kare hatha kabila vit'u zhosi kuumbwa naye ndiye ariye dzulu za kila kit'u...\n",
            "Pred[2]: Uvoro uu ndo fuhubiri kila mut'u ariyelagwa Masihi kwa ut'u wa wo musalabani Uwo ni ut'u wa Ayahudi ...\n",
            "Label[2]: ela sino funamuhubiri Masihi ariyesulubiwa musalabani Uvoro uu unaatsukiza Ayahudi na kwa Ayunani ni...\n",
            "\n",
            "Metrics: BLEU = 8.839, Gen Length = 55.7289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 19658 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 19658 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Mimi ninamuhuma auye kwenu ela dzulu za yo mimi mwenye mwenye...\n",
            "Label[0]: Bai namudzya kwako kaheri kwa vizho muhokere kwani iye be a moyoni mwangu...\n",
            "Pred[1]: P'et'ero na Johana makimulola yuyahu mut'u ariyekala adzaamba...\n",
            "Label[1]: Vikara P'et'ero na Johana marihoona vizho makimuthema dzitso ye mut'u gonya P'et'ero akimwamba Hulol...\n",
            "Pred[2]: Mwenye mut'u akikala ana kit'u kinyume cha mut'u mungine kahi zenu kwanoni mbona munaenderera makosa...\n",
            "Label[2]: Vidze k'amumanya kukala sino at'u a Mulungu fundahukumu hatha malaika H'aya be kala vizho funazhadim...\n",
            "\n",
            "Metrics: BLEU = 8.9132, Gen Length = 55.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 18224 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 18224 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Mut'u yuyu wafuhendya fukakale ahumiki a kilagane kisha kwa at'u kwa kukala si kilagane cha sheria z...\n",
            "Label[0]: Iye nde ariyefwadimira hukale ahumiki a kwakwe kahi za kilagane kisha Kilagane kiki si cha Sheria za...\n",
            "Pred[1]: Vikara wakathi wa P'asaka urihofika Jesu wakwenda Jerusalemu...\n",
            "Label[1]: Gonya wakathi wa sikuk'uu ya Kiyahudi ya P'asaka urihofika hehi Jesu wambuka kwenda Jerusalemu...\n",
            "Pred[2]: Vikara Jesu na anafundzie makwenda Beth'isaida na at'u angine makimureha kipofu mut'u yuyahu mamugut...\n",
            "Label[2]: Bai machenda hatha makifika Beth'isaida na kuko at'u makimurehera Jesu mut'u ariyekala ni kipofu na ...\n",
            "\n",
            "Metrics: BLEU = 8.8844, Gen Length = 55.5806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 18756 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 18756 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Bai aryahu at'u abomu mariokala madzatoa kwa kuluhiro hatha makafwa K'amavipata viryahu Mulungu ariz...\n",
            "Label[0]: At'u aa osi mafwa manakuluhira vingahokala zho marizholagwa ni Mulungu k'amavipatire Mahenda kuzhona...\n",
            "Pred[1]: Ela kumbukira kwa yo zawadi uriyomup'a kukirira na unabii uriwo unawogerwa ni o azhere azhere kuikir...\n",
            "Label[1]: Usikiriche bule cho kigerwacho cha kiroho kiricho ndani mwako Kigerwa kicho wakigerwa wakathi hariho...\n",
            "Pred[2]: Uthawali uriwo unapigana na ye mwenye k'undaindatoa...\n",
            "Label[2]: Ts'i at'ue makidzigazha makundi-makundi gahehago ts'i iyo k'aindatoa...\n",
            "\n",
            "Metrics: BLEU = 8.7647, Gen Length = 56.4514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 19646 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 19646 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Ela Elija ni kwa kukala were k'ahana mut'u kahi za o ache magungu a Iziraeli h'akeye nde ariyehumwa ...\n",
            "Label[0]: Ela ko Elija k'ahumwirwe kwa yoyosi kahi za ao hat'u ha vizho akihumwa kwa mwanamuche gungu kuko Sar...\n",
            "Pred[1]: Kwa vizho manahenda vivi kwa mahendzo na kumanya kukala Mulungu adzanip'a kazi ya kuhubiri wo Uvoro ...\n",
            "Label[1]: Aa mahubirio kwa nia mbidzo manavihenda kwa ut'u wa mahendzo Manamanya kukala Mulungu adzanika haha ...\n",
            "Pred[2]: Ye mkurima ahendaye kazi kwa bidii anafaa kula kwa kwandza kwa mavuno...\n",
            "Label[2]: H'aya fukihala mufano wa muk'urima ni yuyahu ariye ana bidhii na kurima nde ariye anavirya akale wa ...\n",
            "\n",
            "Metrics: BLEU = 8.8872, Gen Length = 55.9028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 19492 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 19492 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Ni here zhomu zha kumala mwiri wa kifwa na undaona hat'u ha ts'i za mbazi haha dzulu...\n",
            "Label[0]: Kwani Hadzihogwa kimba ndo nderi mathungananaho...\n",
            "Pred[1]: Na vizho ndo virizho kahi za kuluhiro ni karakara kidza k'akuna kit'u chochochosi kala k'akihenda ki...\n",
            "Label[1]: Kwa vizho mut'u adziambaye anamukuluhira Mulungu ela ko k'ana rorosi ridzo ahenderaro mut'u kuluhiro...\n",
            "Pred[2]: Vidze aryahu ahumikio madzo kidza mahendzo ga hendani mukumbukira kukala una Bwana ko mulunguni...\n",
            "Label[2]: Vikara ninwi murio muna atumwa hakikishani kukala munaahalirat'o na kwa hachi Musisahau kukala ninwi...\n",
            "\n",
            "Metrics: BLEU = 8.911, Gen Length = 55.9974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 16222 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 16222 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: O at'u makimudzigidzya makiamba K'ahana p'ep'o adziyekuhendya amwalage...\n",
            "Label[0]: O at'u makimwamba Una p'ep'o Ni h'ani amalaye kukwalaga bewe...\n",
            "Pred[1]: Mut'u yuyu ni mut'u kahi za ziya zhosi zha mwiriwe Kidza mwiriwe unagwirira mwiriwe wowosi na uumbe ...\n",
            "Label[1]: Bai lo ludhimi nalo ni here ts'ets'e ya moho lu thele maut'u manji mai na nikumuhendya mut'u akanong...\n",
            "Pred[2]: Munamanya kukala at'u mario ni ai na enye dambi zidzazho munamanya kukala mahendao dambi zao ni kuhe...\n",
            "Label[2]: Kwani mut'u dza yuyu unamanya kukala ana dambi na adzanongeka kidza anadzipiga ulongo mwenye...\n",
            "\n",
            "Metrics: BLEU = 9.1416, Gen Length = 56.8223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 18738 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 18738 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Mukiona zo thabu nirizodzadzipata kwa uso na kidza mukisikira kukala nidzapata thabu nyinji zidzazo ...\n",
            "Label[0]: Vikara nanwi mundadima kuhala muza hamwenga nami kahi za yo k'ondo K'ondo ii ni iyo muriyonona nikii...\n",
            "Pred[1]: Ndo makipiga k'ululu na kuamba P'et'ero ye muthawali wa Ayahudi...\n",
            "Label[1]: Gonya makikwatya kumuvudhya na kumulamusa makiamba K'una ut'u muheshimiwa muthawali wa Ayahudi...\n",
            "Pred[2]: Jesu akienderera kunena akiamba Ndzoni ndani zangu nyosi mut'u adziyechoka kula kahi za wo muzigowe ...\n",
            "Label[2]: Ndzoni kwangu ninwi nyosi musirimao na murio mudzaremererwa ni mizigo nami nindamuoyeza...\n",
            "\n",
            "Metrics: BLEU = 8.8545, Gen Length = 55.7826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 19300 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 19300 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Na vizho ndizho virizhokala virizhokala vindakala dza vizho ao at'u mafufulwao na mwiri wa mbazi nik...\n",
            "Label[0]: Na vizho ndo virizho hatha kahi za kufufuka Wo mwiri uzikwao ni mwiri wa kufwa na kuola ela uryahu u...\n",
            "Pred[1]: Kwa vizho hunakwenda ko ndze ya yo k'anda ili fupate aibu ii ariyokala nayo...\n",
            "Label[1]: Kwa vizho bai nafumuthuwe Jesu kuko ndze ya k'ambi fukapate muthalo wehu kahi za riro hukanwa ariroh...\n",
            "Pred[2]: Na pia kuna miri ya mulunguni na miri ya haha dhuniani Ela udzo wa miri ya mulunguni ni miri mwenga ...\n",
            "Label[2]: Kuna miri ya mulunguni na miri ya dhuniani Udzo wa miri ya mulunguni u vingine na udzo wa miri ya dh...\n",
            "\n",
            "Metrics: BLEU = 9.0952, Gen Length = 55.8312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 18044 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 18044 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Na kwa kukala Mulungu anadima kumup'a baraka nyinji kukira zhosi nanwi mundakala na vinji zhosi zhen...\n",
            "Label[0]: Na Mulungu ana wadimi wa kumup'a ninwi zaidhi ya zho mumalazho kwamba siku zosi mukale na vit'u zha ...\n",
            "Pred[1]: Ye mubomu wa shikari akimudzigidzya akiamba Bwana muhumiki wangu ni mukongo sana kitandani k'adima k...\n",
            "Label[1]: Bwana muhumiki wangu ni mukongo sana Vilungozhe zhaholoza na analumwa k'azhadimikika...\n",
            "Pred[2]: Bai ninwi mwasikira zho zhaambwa kukala Musikale na ut'u uriwo unaaaika ela hat'u ha vizho usitsuhe ...\n",
            "Label[2]: Mwasikira kukala haho kapindi at'u maambwa Ukiapa mbere za Bwana usilahe ulongo bule thimiza kila ki...\n",
            "\n",
            "Metrics: BLEU = 9.1919, Gen Length = 56.4847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 21158 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 21158 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Daudi mwenye anamwiha Masihi Bwana vino be anadimadze kukala mwana wa Daudi na at'u anji makimusirik...\n",
            "Label[0]: Vikikala Daudi mwenye amwiha Bwana anadimadze kukala mwanawe Bai wo muthunganano mubomu ukimusirikiz...\n",
            "Pred[1]: K'uzhona zho nikikala mut'u wa Ayahudi ili kwamba ao mario were k'amathawaliwa ni Sheria nakala here...\n",
            "Label[1]: Kwa Ayahudi nakala here Muyahudi ili niavuhe Ayahudi Na ingahokala mimi si ts'ini za sheria yao naka...\n",
            "Pred[2]: Kwa vizho bai mut'u yoyosi ariye k'amundathubu akidzigidzya haraka nindakudzirana na aa nao kwa mush...\n",
            "Label[2]: H'aya thubuni kwani kala k'amundathubu nadza kare niapige ao mathuwao mafundisho mai na wo mushu umb...\n",
            "\n",
            "Metrics: BLEU = 9.2663, Gen Length = 55.0818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 21518 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 21518 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Vikara Jesu arihokala akwendani ko Murima wa Mizeituni achenda hatha achangira kahi za wo murima wa ...\n",
            "Label[0]: Ela Jesu ye achenda kahi za Murima wa Mizeituni...\n",
            "Pred[1]: Bai aryahu mabaharia makigoha kwamba hupige go mawe makigatsuha nanga ne za madzi na makivoya makivo...\n",
            "Label[1]: Bai kwa ut'u wa kuogoha p'ore makagwizanywa na ho mbararani mahala zo nanga ne na makizitharamusha m...\n",
            "Pred[2]: Vikara kwereko kuko kundi bomu ra at'u mariokala mananena na Jesu kisiri-siri-siri-dzakala makiamba ...\n",
            "Label[2]: Kidza kahi za ro kundi mwere muna masumuriro ga kinjama-njama dzuluze Angine were manaamba Ni mut'u ...\n",
            "\n",
            "Metrics: BLEU = 9.4554, Gen Length = 55.4054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 17196 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 17196 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Ye muhumiki wa hiri akidzigidzya akiamba Bwana na wo mfumowe mwenga wa dhahabu nidzazhala ts'ano...\n",
            "Label[0]: Gonya ye wa hiri akidza akiamba Bwana iryahu shilingi yo ya dhahabu yareha faidha ya shilingi nyingi...\n",
            "Pred[1]: Nami namwandhikira kahi za baruwa yangu kwamba ushirikaane na at'u ahendzao...\n",
            "Label[1]: Ela vikara ut'u niriwokala ninamwambira ni uu kukala musigwirane na mut'u ariye ana dzina ra ndugu g...\n",
            "Pred[2]: Kwani Maoro ganaamba kukala Burahemu ana ana airi mameye wa mulume mumwenga were ni mwanamuche mutum...\n",
            "Label[2]: Iyo inaamba kukala Burahemu were ana ana airi a kilume Mumwenga wamuzhala na mwanamuche mutumwa na m...\n",
            "\n",
            "Metrics: BLEU = 9.4334, Gen Length = 55.9297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 19206 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 19206 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Kwa vizho bai o at'u k'amakubalianire dzulu za Jesu...\n",
            "Label[0]: Kwa vizho at'u makigazhikana kwa ut'u wa Jesu...\n",
            "Pred[1]: Ela ninwi nanwi munafundisha kukala mut'u anadima kumwamba babaye na mameye vit'u nindazhala ili nid...\n",
            "Label[1]: Ela nwi munafundisha kukala mut'u akihala kit'u adimacho kumup'a babaye hedu mameye na akakala adzam...\n",
            "Pred[2]: Iye nde Mulungu ahuthizhaye ili kwamba kukirira kwa Jesu Masihi apate nguvu za nguma na wadimi wosi ...\n",
            "Label[2]: Kwa iye Mulungu h'akeye ariye nde mwokoli wehu nguma na ubomu na wadimi navikale kwakwe kukirira kwa...\n",
            "\n",
            "Metrics: BLEU = 9.2443, Gen Length = 55.7916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Found 19510 token IDs outside vocab range. Replacing with pad token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (782, 128), Labels shape: (782, 128)\n",
            "Tokenizer vocabulary size: 58950\n",
            "Found 19510 token IDs outside vocab range. Replacing with pad token.\n",
            "\n",
            "===== PREDICTION EXAMPLES =====\n",
            "Pred[0]: Ela Elija ninaamba kukala k'ahana mut'u kahi za o ache magungu a Iziraeli h'akeye nde ariyehumwa kwa...\n",
            "Label[0]: Ela ko Elija k'ahumwirwe kwa yoyosi kahi za ao hat'u ha vizho akihumwa kwa mwanamuche gungu kuko Sar...\n",
            "Pred[1]: Kwa vizho bai mukirya mukahe hedu kunwa kikombe cha Bwana kwa ngira ambayo ye Bwana k'ana makosa ga ...\n",
            "Label[1]: Kwa ut'u uwo wenye bai mut'u yoyosi andiyerya mukahe hedu kunwa kikombe cha Bwana kahi za ngira isiy...\n",
            "Pred[2]: Bai aryahu mabaharia makigoha kwamba hupige go mawe makigatsuha nanga ne za madzi na makivoya makiuy...\n",
            "Label[2]: Bai kwa ut'u wa kuogoha p'ore makagwizanywa na ho mbararani mahala zo nanga ne na makizitharamusha m...\n",
            "\n",
            "Metrics: BLEU = 9.8268, Gen Length = 55.5908\n"
          ]
        }
      ]
    }
  ]
}